# NeMo RL Examples

This directory contains working examples and tutorials for NeMo RL.

## Quick Start Examples

- [SFT Training](run_sft.py) - Supervised Fine-Tuning example
- [DPO Training](run_dpo.py) - Direct Preference Optimization example  
- [GRPO Training](run_grpo_math.py) - Group Relative Policy Optimization with math environment
- [GRPO Sliding Puzzle](run_grpo_sliding_puzzle.py) - GRPO with sliding puzzle environment
- [Evaluation](run_eval.py) - Model evaluation example

## Advanced Examples

- [Custom Parallel Training](custom_parallel.py) - Custom parallel training implementation
- [Model Converters](converters/) - Convert models between different formats

## Configuration Examples

- [Configs](configs/) - Training configuration examples
- [Prompts](prompts/) - Example prompts for different tasks

## Getting Started

1. Choose an example that matches your use case
2. Review the configuration in the `configs/` directory
3. Run the example with your own data or environment
4. Modify parameters to suit your specific needs

For more detailed guides, see the [documentation](../docs/). 