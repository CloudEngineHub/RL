{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-12 11:31:22 [__init__.py:256] Automatically detected platform cuda.\n",
            "Loading tokenizers...\n",
            "Tokenizers loaded. Models will be loaded on-demand to save memory.\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import gc # For garbage collection\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# --- Configuration ---\n",
        "LLADA_MODEL_NAME = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
        "LLAMA3_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- Load Tokenizers ---\n",
        "print(\"Loading tokenizers...\")\n",
        "llada_tokenizer = AutoTokenizer.from_pretrained(LLADA_MODEL_NAME, trust_remote_code=True)\n",
        "llama3_tokenizer = AutoTokenizer.from_pretrained(LLAMA3_MODEL_NAME)\n",
        "\n",
        "# Add a padding token to Llama3 tokenizer if it doesn't exist\n",
        "if llama3_tokenizer.pad_token is None:\n",
        "    llama3_tokenizer.pad_token = llama3_tokenizer.eos_token\n",
        "# Note: The model config for pad_token_id will be set when the model is loaded.\n",
        "    \n",
        "print(f\"Tokenizers loaded. Models will be loaded on-demand to save memory.\")\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaDA simple generation function loaded.\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def get_num_transfer_tokens(mask_index, steps):\n",
        "    \"\"\"\n",
        "    Computes the number of tokens to transition at each step of the diffusion process.\n",
        "    This function is copied from the generation.ipynb notebook.\n",
        "    \"\"\"\n",
        "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
        "    base = mask_num // steps\n",
        "    remainder = mask_num % steps\n",
        "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
        "    for i in range(mask_num.size(0)):\n",
        "        num_transfer_tokens[i, :remainder[i]] += 1\n",
        "    return num_transfer_tokens\n",
        "\n",
        "@torch.no_grad()\n",
        "def llada_generate_simple(model, tokenizer, prompt_text, gen_length=512, block_length=128):\n",
        "    \"\"\"\n",
        "    A simplified LLaDA generation function using a basic block generation approach.\n",
        "    It does not include advanced sampling like Gumbel noise, CFG, or temperature.\n",
        "    \"\"\"\n",
        "    mask_id = llada_tokenizer.vocab.get(\"[MASK]\")  # Find MASK token id\n",
        "    if mask_id is None:\n",
        "        # Fallback if specific [MASK] isn't in vocab, which is unlikely for LLaDA\n",
        "        mask_id = llada_tokenizer.mask_token_id or 0\n",
        "\n",
        "    # Format prompt\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
        "    prompt_len = input_ids.shape[1]\n",
        "\n",
        "    # Initialize sequence with masks\n",
        "    x = torch.full((1, prompt_len + gen_length), mask_id, dtype=torch.long, device=DEVICE)\n",
        "    x[:, :prompt_len] = input_ids.clone()\n",
        "\n",
        "    mask_index_original = (x == mask_id)\n",
        "    \n",
        "    # Simple block generation\n",
        "    num_blocks = (gen_length + block_length - 1) // block_length  # Ceiling division\n",
        "    steps = 4 # Fewer steps for speed benchmark\n",
        "\n",
        "    for i in range(steps):\n",
        "        mask_index = (x == mask_id)\n",
        "        if not mask_index.any():\n",
        "            break\n",
        "\n",
        "        logits = model(x).logits\n",
        "        \n",
        "        # Simple greedy decoding\n",
        "        x0 = torch.argmax(logits, dim=-1)\n",
        "        \n",
        "        # Determine which tokens to unmask based on confidence (softmax probability)\n",
        "        p = torch.softmax(logits, dim=-1)\n",
        "        x0_p = torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)).squeeze(-1)\n",
        "        \n",
        "        confidence = torch.where(mask_index, x0_p, -torch.inf)\n",
        "        \n",
        "        # Determine number of tokens to reveal in this step\n",
        "        num_to_reveal = (mask_index_original.sum() // steps) + 1\n",
        "        \n",
        "        _, top_indices = torch.topk(confidence.view(-1), k=int(num_to_reveal))\n",
        "        \n",
        "        x.view(-1)[top_indices] = x0.view(-1)[top_indices]\n",
        "\n",
        "    # Final greedy fill for any remaining masks\n",
        "    mask_index = (x == mask_id)\n",
        "    if mask_index.any():\n",
        "        logits = model(x).logits\n",
        "        x0 = torch.argmax(logits, dim=-1)\n",
        "        x = torch.where(mask_index, x0, x)\n",
        "\n",
        "    return tokenizer.batch_decode(x[:, prompt_len:], skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"LLaDA simple generation function loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama3 generation function loaded.\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def llama3_generate(model, tokenizer, prompt_text, gen_length=512):\n",
        "    \"\"\"\n",
        "    Standard autoregressive generation for Llama3.\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    # Generate text\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=gen_length,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=False  # Use greedy decoding for speed comparison\n",
        "    )\n",
        "    \n",
        "    # Decode the generated tokens\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "print(\"Llama3 generation function loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM generation function loaded.\n"
          ]
        }
      ],
      "source": [
        "def vllm_generate(model, tokenizer, prompt_text, gen_length=512):\n",
        "    \"\"\"\n",
        "    Generates text using the vLLM engine.\n",
        "    \"\"\"\n",
        "    # The tokenizer is used for creating the prompt, but vLLM handles tokenization internally\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "    \n",
        "    sampling_params = SamplingParams(\n",
        "        n=1,\n",
        "        temperature=0.0,\n",
        "        max_tokens=gen_length,\n",
        "    )\n",
        "    \n",
        "    # Generate text\n",
        "    outputs = model.generate(prompt, sampling_params)\n",
        "    \n",
        "    # Return the generated text from the first output\n",
        "    return outputs[0].outputs[0].text\n",
        "\n",
        "print(\"vLLM generation function loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmarking framework loaded.\n"
          ]
        }
      ],
      "source": [
        "# --- Benchmarking Framework ---\n",
        "PROMPT_TEXT = \"Explain the theory of general relativity in a few paragraphs.\"\n",
        "GENERATION_LENGTHS = [512, 768, 1024]\n",
        "NUM_TRIALS = 3  # Number of times to run each test to get an average\n",
        "\n",
        "def run_benchmark(model_name, gen_function, tokenizer, gen_length, **kwargs):\n",
        "    \"\"\"\n",
        "    Runs the generation benchmark for a given model and returns timing statistics.\n",
        "    \"\"\"\n",
        "    total_time = 0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    # Warm-up run\n",
        "    print(f\"  Warm-up run for {gen_length} tokens...\")\n",
        "    gen_function(\n",
        "        model=kwargs.get('model'), \n",
        "        tokenizer=tokenizer, \n",
        "        prompt_text=PROMPT_TEXT, \n",
        "        gen_length=gen_length,\n",
        "        **kwargs.get('func_args', {})\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Timed trials\n",
        "    for i in range(NUM_TRIALS):\n",
        "        print(f\"  Trial {i+1}/{NUM_TRIALS} for {gen_length} tokens...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        generated_text = gen_function(\n",
        "            model=kwargs.get('model'), \n",
        "            tokenizer=tokenizer, \n",
        "            prompt_text=PROMPT_TEXT, \n",
        "            gen_length=gen_length,\n",
        "            **kwargs.get('func_args', {})\n",
        "        )\n",
        "        \n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        \n",
        "        elapsed_time = end_time - start_time\n",
        "        num_tokens = len(tokenizer.encode(generated_text))\n",
        "        \n",
        "        total_time += elapsed_time\n",
        "        total_tokens += num_tokens\n",
        "\n",
        "    avg_time = total_time / NUM_TRIALS\n",
        "    avg_tokens = total_tokens / NUM_TRIALS\n",
        "    tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Gen Length\": gen_length,\n",
        "        \"Avg Time (s)\": avg_time,\n",
        "        \"Avg Tokens\": avg_tokens,\n",
        "        \"Tokens/Sec\": tokens_per_sec\n",
        "    }\n",
        "\n",
        "print(\"Benchmarking framework loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading LLaDA model: GSAI-ML/LLaDA-8B-Instruct...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9e75a342c904edf8d967b836c9061ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaDA model loaded.\n",
            "\\n==================== Benchmarking LLaDA for 512 tokens ====================\n",
            "  Warm-up run for 512 tokens...\n",
            "  Trial 1/3 for 512 tokens...\n",
            "  Trial 2/3 for 512 tokens...\n",
            "  Trial 3/3 for 512 tokens...\n",
            "\\n==================== Benchmarking LLaDA for 768 tokens ====================\n",
            "  Warm-up run for 768 tokens...\n",
            "  Trial 1/3 for 768 tokens...\n",
            "  Trial 2/3 for 768 tokens...\n",
            "  Trial 3/3 for 768 tokens...\n",
            "\\n==================== Benchmarking LLaDA for 1024 tokens ====================\n",
            "  Warm-up run for 1024 tokens...\n",
            "  Trial 1/3 for 1024 tokens...\n",
            "  Trial 2/3 for 1024 tokens...\n",
            "  Trial 3/3 for 1024 tokens...\n",
            "\\nClearing LLaDA model from memory...\n",
            "LLaDA model cleared.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# --- Run Benchmarks ---\n",
        "results_list = []\n",
        "\n",
        "# --- LLaDA Benchmark ---\n",
        "print(f\"Loading LLaDA model: {LLADA_MODEL_NAME}...\")\n",
        "llada_model = AutoModel.from_pretrained(\n",
        "    LLADA_MODEL_NAME, \n",
        "    trust_remote_code=True, \n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(DEVICE).eval()\n",
        "print(\"LLaDA model loaded.\")\n",
        "\n",
        "llada_params = {'block_length': 128}\n",
        "for length in GENERATION_LENGTHS:\n",
        "    print(f\"\\\\n{'='*20} Benchmarking LLaDA for {length} tokens {'='*20}\")\n",
        "    llada_results = run_benchmark(\n",
        "        \"LLaDA-8B\",\n",
        "        llada_generate_simple,\n",
        "        llada_tokenizer,\n",
        "        length,\n",
        "        model=llada_model,\n",
        "        func_args=llada_params\n",
        "    )\n",
        "    results_list.append(llada_results)\n",
        "\n",
        "# Clear LLaDA model from memory\n",
        "print(\"\\\\nClearing LLaDA model from memory...\")\n",
        "del llada_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"LLaDA model cleared.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nLoading Llama3 model with vLLM: meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "INFO 08-12 11:32:13 [config.py:583] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 08-12 11:32:13 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 08-12 11:32:15 [utils.py:2148] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.\n",
            "INFO 08-12 11:32:25 [__init__.py:256] Automatically detected platform cuda.\n",
            "INFO 08-12 11:32:31 [core.py:53] Initializing a V1 LLM engine (v0.8.0) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 08-12 11:32:39 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7980a9c0e000>\n",
            "INFO 08-12 11:32:47 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 08-12 11:32:47 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
            "INFO 08-12 11:32:47 [gpu_model_runner.py:1128] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "WARNING 08-12 11:32:47 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 08-12 11:32:47 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.23it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]\n",
            "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.85it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.68it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.60it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-12 11:32:50 [loader.py:429] Loading weights took 2.61 seconds\n",
            "INFO 08-12 11:32:50 [gpu_model_runner.py:1140] Model loading took 14.9595 GB and 3.535962 seconds\n",
            "ERROR 08-12 11:32:59 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):\n",
            "ERROR 08-12 11:32:59 [core.py:340]   File \"/home/mahan/.venvs/default/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 332, in run_engine_core\n",
            "ERROR 08-12 11:32:59 [core.py:340]     engine_core = EngineCoreProc(*args, **kwargs)\n",
            "ERROR 08-12 11:32:59 [core.py:340]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 08-12 11:32:59 [core.py:340]   File \"/home/mahan/.venvs/default/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 287, in __init__\n",
            "ERROR 08-12 11:32:59 [core.py:340]     super().__init__(vllm_config, executor_class, log_stats)\n",
            "ERROR 08-12 11:32:59 [core.py:340]   File \"/home/mahan/.venvs/default/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 62, in __init__\n",
            "ERROR 08-12 11:32:59 [core.py:340]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\n",
            "ERROR 08-12 11:32:59 [core.py:340]                                      ^^^^^^^^^^^CRITICAL 08-12 11:32:59 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n"
          ]
        }
      ],
      "source": [
        "# --- Llama3 vLLM Benchmark ---\n",
        "print(f\"\\\\nLoading Llama3 model with vLLM: {LLAMA3_MODEL_NAME}...\")\n",
        "# vLLM handles device placement automatically\n",
        "llama3_model = LLM(model=LLAMA3_MODEL_NAME, trust_remote_code=True)\n",
        "print(\"Llama3 vLLM model loaded.\")\n",
        "\n",
        "for length in GENERATION_LENGTHS:\n",
        "    print(f\"\\\\n{'='*20} Benchmarking Llama3 (vLLM) for {length} tokens {'='*20}\")\n",
        "    llama3_results = run_benchmark(\n",
        "        \"Llama-3-8B (vLLM)\",\n",
        "        vllm_generate,\n",
        "        llama3_tokenizer,\n",
        "        length,\n",
        "        model=llama3_model\n",
        "    )\n",
        "    results_list.append(llama3_results)\n",
        "\n",
        "# Clear Llama3 model from memory\n",
        "print(\"\\\\nClearing Llama3 model from memory...\")\n",
        "del llama3_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Llama3 model cleared.\")\n",
        "\n",
        "# Convert results to a DataFrame for easy analysis\n",
        "results_df = pd.DataFrame(results_list)\n",
        "\n",
        "print(\"\\\\nBenchmark Complete!\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Visualize Results ---\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "# Plotting the data\n",
        "pivot_df = results_df.pivot(index='Gen Length', columns='Model', values='Tokens/Sec')\n",
        "pivot_df.plot(kind='bar', ax=ax, width=0.4)\n",
        "\n",
        "# Formatting the plot\n",
        "ax.set_title('LLaDA-8B vs. Llama-3-8B Generation Speed', fontsize=16)\n",
        "ax.set_xlabel('Generation Length (Number of Tokens)', fontsize=12)\n",
        "ax.set_ylabel('Tokens per Second', fontsize=12)\n",
        "ax.tick_params(axis='x', rotation=0)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "ax.legend(title='Model')\n",
        "\n",
        "# Adding labels on top of the bars\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=10, padding=3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "default",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
