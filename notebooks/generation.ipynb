{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device).eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n",
    "\n",
    "print(\"Core LLaDA functions loaded: add_gumbel_noise, get_num_transfer_tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df103ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    '''\n",
    "    Args:\n",
    "        model: Mask predictor.\n",
    "        prompt: A tensor of shape (1, L).\n",
    "        steps: Sampling steps, less than or equal to gen_length.\n",
    "        gen_length: Generated answer length.\n",
    "        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n",
    "        temperature: Categorical distribution sampling temperature.\n",
    "        cfg_scale: Unsupervised classifier-free guidance scale.\n",
    "        remasking: Remasking strategy. 'low_confidence' or 'random'.\n",
    "        mask_id: The token id of [MASK] is 126336.\n",
    "    '''\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "        for i in range(steps):\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l\n",
    "\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "    return x\n",
    "\n",
    "# Constants for LLaDA model\n",
    "MASK_ID = 126336  # The token id of [MASK] in LLaDA tokenizer\n",
    "\n",
    "print(\"LLaDA generate function loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703419ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompts for testing different LLaDA strategies\n",
    "sample_prompts = [\n",
    "    \"What do you think the future of artificial intelligence will look like?\",\n",
    "    \"Can you write me a short science fiction story about space exploration?\",\n",
    "    \"I'm struggling to understand quantum computing. Can you explain it in simple terms?\",\n",
    "    \"Could you help me write a creative story about a robot learning to feel emotions?\",\n",
    "    \"What are some practical solutions we could implement to address climate change?\",\n",
    "    \"How does machine learning actually work under the hood?\",\n",
    "    \"I need help planning a healthy meal prep routine for the week\",\n",
    "    \"What's the best way to learn a new programming language as a beginner?\"\n",
    "]\n",
    "\n",
    "print(\"Sample prompts loaded:\")\n",
    "for i, prompt in enumerate(sample_prompts, 1):\n",
    "    print(f\"{i}. {prompt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd83119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llada_generate_with_chat_template(prompt_text: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to generate text using LLaDA with proper chat formatting\n",
    "    \"\"\"\n",
    "    # Format prompt for instruct model\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    \n",
    "    # Generate using LLaDA\n",
    "    output = generate(model, input_ids, **kwargs)\n",
    "    \n",
    "    # Decode only the generated part (excluding input)\n",
    "    generated_text = tokenizer.batch_decode(output[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    return {\n",
    "        'input': formatted_prompt,\n",
    "        'output': generated_text,\n",
    "        'full_sequence': tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    }\n",
    "\n",
    "# Test basic generation\n",
    "test_prompt = sample_prompts[0]\n",
    "print(f\"Testing prompt: '{test_prompt}'\")\n",
    "\n",
    "result = llada_generate_with_chat_template(\n",
    "    test_prompt, \n",
    "    steps=64, \n",
    "    gen_length=64, \n",
    "    block_length=32, \n",
    "    temperature=0.0,\n",
    "    remasking='low_confidence'\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated: {result['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different LLaDA sampling strategies\n",
    "def explore_llada_strategies(prompt_text: str):\n",
    "    \"\"\"\n",
    "    Compare different parameter settings for LLaDA generation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPLORING LLaDA STRATEGIES FOR: '{prompt_text}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            'name': 'Deterministic (temp=0.0)',\n",
    "            'params': {'steps': 128, 'gen_length': 64, 'block_length': 32, 'temperature': 0.0, 'remasking': 'low_confidence'}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Low Temperature (temp=0.5)',\n",
    "            'params': {'steps': 64, 'gen_length': 64, 'block_length': 16, 'temperature': 0.5, 'remasking': 'low_confidence'}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Random Remasking',\n",
    "            'params': {'steps': 128, 'gen_length': 64, 'block_length': 32, 'temperature': 0.0, 'remasking': 'random'}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Semi-Autoregressive (small blocks)',\n",
    "            'params': {'steps': 32, 'gen_length': 64, 'block_length': 8, 'temperature': 0.0, 'remasking': 'low_confidence'}\n",
    "        },\n",
    "        {\n",
    "            'name': 'With CFG Guidance',\n",
    "            'params': {'steps': 128, 'gen_length': 64, 'block_length': 32, 'temperature': 0.0, 'cfg_scale': 1.5, 'remasking': 'low_confidence'}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n🔸 {strategy['name']}\")\n",
    "        try:\n",
    "            result = llada_generate_with_chat_template(prompt_text, **strategy['params'])\n",
    "            results[strategy['name']] = result['output']\n",
    "            print(f\"✅ Output: {result['output'][:120]}{'...' if len(result['output']) > 120 else ''}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            results[strategy['name']] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with different prompts\n",
    "for prompt in sample_prompts[:2]:  # Test first 2 prompts\n",
    "    explore_llada_strategies(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Analysis: Understanding LLaDA's key parameters\n",
    "def analyze_block_length_effect():\n",
    "    \"\"\"\n",
    "    Analyze how block_length affects generation patterns\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BLOCK LENGTH ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    prompt = \"Explain how machine learning works\"\n",
    "    block_lengths = [8, 16, 32, 64]\n",
    "    \n",
    "    for block_length in block_lengths:\n",
    "        print(f\"\\n🔹 Block Length: {block_length}\")\n",
    "        try:\n",
    "            result = llada_generate_with_chat_template(\n",
    "                prompt,\n",
    "                steps=64,\n",
    "                gen_length=64,\n",
    "                block_length=block_length,\n",
    "                temperature=0.0,\n",
    "                remasking='low_confidence'\n",
    "            )\n",
    "            print(f\"Output: {result['output'][:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "def analyze_temperature_effect():\n",
    "    \"\"\"\n",
    "    Analyze how temperature affects generation diversity\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEMPERATURE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    prompt = \"Write a creative story about\"\n",
    "    temperatures = [0.0, 0.3, 0.7, 1.0]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n🔹 Temperature: {temp}\")\n",
    "        try:\n",
    "            result = llada_generate_with_chat_template(\n",
    "                prompt,\n",
    "                steps=64,\n",
    "                gen_length=64,\n",
    "                block_length=32,\n",
    "                temperature=temp,\n",
    "                remasking='low_confidence'\n",
    "            )\n",
    "            print(f\"Output: {result['output'][:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Run analyses\n",
    "analyze_block_length_effect()\n",
    "analyze_temperature_effect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced LLaDA Features: CFG and Remasking Strategies\n",
    "def compare_remasking_strategies():\n",
    "    \"\"\"\n",
    "    Compare low_confidence vs random remasking strategies\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REMASKING STRATEGY COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    prompt = \"The benefits of renewable energy include\"\n",
    "    \n",
    "    strategies = ['low_confidence', 'random']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n🔹 Remasking: {strategy}\")\n",
    "        try:\n",
    "            result = llada_generate_with_chat_template(\n",
    "                prompt,\n",
    "                steps=128,\n",
    "                gen_length=80,\n",
    "                block_length=40,\n",
    "                temperature=0.0,\n",
    "                remasking=strategy\n",
    "            )\n",
    "            print(f\"Output: {result['output']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "def explore_cfg_guidance():\n",
    "    \"\"\"\n",
    "    Explore Classifier-Free Guidance effects\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFIER-FREE GUIDANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    prompt = \"In the future, artificial intelligence will\"\n",
    "    cfg_scales = [0.0, 1.0, 2.0, 3.0]\n",
    "    \n",
    "    for cfg_scale in cfg_scales:\n",
    "        print(f\"\\n🔹 CFG Scale: {cfg_scale}\")\n",
    "        try:\n",
    "            result = llada_generate_with_chat_template(\n",
    "                prompt,\n",
    "                steps=64,\n",
    "                gen_length=64,\n",
    "                block_length=32,\n",
    "                temperature=0.0,\n",
    "                cfg_scale=cfg_scale,\n",
    "                remasking='low_confidence'\n",
    "            )\n",
    "            print(f\"Output: {result['output'][:120]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Run advanced feature exploration\n",
    "compare_remasking_strategies()\n",
    "explore_cfg_guidance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Examples: Using LLaDA for Different Tasks\n",
    "def demonstrate_practical_applications():\n",
    "    \"\"\"\n",
    "    Show LLaDA applied to different types of tasks\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRACTICAL LLaDA APPLICATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    applications = [\n",
    "        {\n",
    "            'task': 'Question Answering',\n",
    "            'prompt': 'What are the main causes of climate change?',\n",
    "            'params': {'steps': 128, 'gen_length': 100, 'block_length': 25, 'temperature': 0.0}\n",
    "        },\n",
    "        {\n",
    "            'task': 'Creative Writing',\n",
    "            'prompt': 'Write a short poem about the ocean',\n",
    "            'params': {'steps': 64, 'gen_length': 80, 'block_length': 20, 'temperature': 0.7}\n",
    "        },\n",
    "        {\n",
    "            'task': 'Code Explanation',\n",
    "            'prompt': 'Explain what this Python function does: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)',\n",
    "            'params': {'steps': 96, 'gen_length': 120, 'block_length': 30, 'temperature': 0.2}\n",
    "        },\n",
    "        {\n",
    "            'task': 'Mathematical Problem',\n",
    "            'prompt': 'Solve this step by step: If a train travels 120 km in 2 hours, what is its average speed?',\n",
    "            'params': {'steps': 128, 'gen_length': 100, 'block_length': 25, 'temperature': 0.0}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for app in applications:\n",
    "        print(f\"\\n🎯 {app['task']}\")\n",
    "        print(f\"Prompt: {app['prompt'][:60]}...\")\n",
    "        try:\n",
    "            result = llada_generate_with_chat_template(\n",
    "                app['prompt'], \n",
    "                remasking='low_confidence',\n",
    "                **app['params']\n",
    "            )\n",
    "            print(f\"Output: {result['output']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "demonstrate_practical_applications()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde516ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Tips for LLaDA Usage\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 LLaDA DIFFUSION LANGUAGE MODEL - COMPLETE IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "✅ IMPLEMENTED FEATURES:\n",
    "\n",
    "1. 🎯 Core LLaDA Functions:\n",
    "   • add_gumbel_noise() - Gumbel sampling for categorical distributions\n",
    "   • get_num_transfer_tokens() - Linear noise schedule implementation\n",
    "   • generate() - Full LLaDA generation with all features\n",
    "\n",
    "2. 🔧 Key Parameters:\n",
    "   • steps: Number of diffusion steps (32-128)\n",
    "   • gen_length: Output sequence length\n",
    "   • block_length: Semi-autoregressive block size\n",
    "   • temperature: Sampling randomness (0.0-1.0+)\n",
    "   • cfg_scale: Classifier-free guidance strength\n",
    "   • remasking: 'low_confidence' or 'random'\n",
    "\n",
    "3. 🎨 Sampling Strategies:\n",
    "   • Deterministic (temp=0.0) - Consistent outputs\n",
    "   • Stochastic (temp>0.0) - Creative/diverse outputs  \n",
    "   • CFG Guidance - Enhanced instruction following\n",
    "   • Semi-autoregressive - Faster generation with blocks\n",
    "\n",
    "4. 📊 Analysis Tools:\n",
    "   • Parameter comparison functions\n",
    "   • Practical application examples\n",
    "   • Performance analysis across tasks\n",
    "\n",
    "💡 USAGE TIPS:\n",
    "\n",
    "• For factual Q&A: Use temp=0.0, low block_length\n",
    "• For creative tasks: Use temp=0.5-1.0, larger blocks  \n",
    "• For instruction following: Add cfg_scale=1.5-3.0\n",
    "• For speed: Reduce steps, increase block_length\n",
    "• For quality: Increase steps, use 'low_confidence' remasking\n",
    "\n",
    "🔗 Based on LLaDA paper: arXiv:2409.02908\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🎉 Ready to explore diffusion language generation!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 🚀 LLaDA Diffusion Language Model - Complete Implementation\n",
    "\n",
    "## What is LLaDA?\n",
    "\n",
    "**LLaDA (Large Language Diffusion Models)** is a state-of-the-art approach that applies diffusion processes to language generation. Unlike autoregressive models that generate tokens sequentially, LLaDA uses a diffusion process to iteratively refine masked text.\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### **🎯 Key Functions:**\n",
    "\n",
    "1. **`add_gumbel_noise()`** - Implements Gumbel max sampling for categorical distributions\n",
    "   - Uses float64 precision for quality (per arXiv:2409.02908)\n",
    "   - Controls randomness in token selection\n",
    "\n",
    "2. **`get_num_transfer_tokens()`** - Manages linear noise schedule\n",
    "   - Ensures consistent token transitions per step\n",
    "   - Handles remainder distribution for uniform sampling\n",
    "\n",
    "3. **`generate()`** - Main diffusion generation function\n",
    "   - Block-based generation for efficiency\n",
    "   - Supports CFG and multiple remasking strategies\n",
    "\n",
    "### **🔧 Critical Parameters:**\n",
    "\n",
    "- **`steps`**: Diffusion steps (32-128) - More steps = higher quality\n",
    "- **`gen_length`**: Output sequence length\n",
    "- **`block_length`**: Semi-autoregressive blocks - Smaller = more parallel\n",
    "- **`temperature`**: Sampling randomness (0.0-1.0+)\n",
    "- **`cfg_scale`**: Classifier-free guidance strength (0.0-3.0+)\n",
    "- **`remasking`**: Strategy ('low_confidence' or 'random')\n",
    "- **`mask_id`**: 126336 (LLaDA tokenizer mask token)\n",
    "\n",
    "### **🎨 Sampling Strategies:**\n",
    "\n",
    "1. **Deterministic** (`temp=0.0`) - Consistent, factual outputs\n",
    "2. **Stochastic** (`temp>0.0`) - Creative, diverse generation\n",
    "3. **CFG Guidance** - Enhanced instruction following\n",
    "4. **Low-Confidence Remasking** - Iterative quality improvement\n",
    "5. **Semi-Autoregressive** - Fast generation with variable block sizes\n",
    "\n",
    "## Optimal Usage Patterns\n",
    "\n",
    "| Task Type | Temperature | Block Length | CFG Scale | Steps |\n",
    "|-----------|-------------|--------------|-----------|-------|\n",
    "| **Q&A/Facts** | 0.0 | 16-32 | 0.0-1.0 | 128 |\n",
    "| **Creative Writing** | 0.5-1.0 | 20-40 | 1.0-2.0 | 64-96 |\n",
    "| **Code/Math** | 0.0-0.2 | 25-32 | 0.0 | 128 |\n",
    "| **Fast Generation** | 0.0 | 64 | 0.0 | 32-64 |\n",
    "\n",
    "## Key Advantages\n",
    "\n",
    "- **Non-autoregressive**: Can refine any part of the sequence\n",
    "- **Iterative refinement**: Multiple passes improve quality\n",
    "- **Block parallelism**: Faster than pure autoregressive\n",
    "- **CFG support**: Better instruction following\n",
    "- **Flexible control**: Fine-grained parameter tuning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
