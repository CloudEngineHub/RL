# Evaluation Configuration
generation:
  backend: "vllm" # only vllm is supported for evaluation
  max_new_tokens: ${generation.vllm_cfg.max_model_len}
  temperature: 0.6
  top_p: 0.95 # was 1 but doesn't compare with ido's
  top_k: -1 # disable
  num_prompts_per_step: -1 # -1 means pass all prompts at once
  model_name: "nvidia/Llama-3.1-Nemotron-Nano-8B-v1"
  stop_token_ids: null
  stop_strings: null
  vllm_cfg:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_model_len: 24000 #2048
    max_num_batched_tokens: 2
    seed: 9388
tokenizer:
  name: ${generation.model_name} ## specify if you'd like to use a tokenizer different from the model's default
  chat_template: "default"

data:
  max_input_seq_length: ${generation.vllm_cfg.max_model_len} # useless since we directly use prompts in evaluation
  prompt_file: "examples/prompts/cot_nano_user.txt"
  #prompt_file: null
  system_prompt_file: "examples/prompts/cot_nano_system.txt"
  dataset_name: "HuggingFaceH4/aime_2024"
  dataset_key: "train"
  problem_key: "problem"
  solution_key: "answer"
  planted_thinking_prompt : "<think>" #none #"<think>Thinking . . . . . . . . . . . . . . . . ."

env:
  math:
    num_workers: 8

cluster:
  gpus_per_node: 6
  num_nodes: 1

debug:
  outfile: none